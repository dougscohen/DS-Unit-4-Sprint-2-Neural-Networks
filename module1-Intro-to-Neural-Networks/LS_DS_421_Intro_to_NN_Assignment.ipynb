{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_421_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dougscohen/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### Input Layer: The input layer is the layer that contains the initial data that is used for the neural network (NN). \n",
        "### Hidden Layer: The layer between the input and output layers that does all the computation within the NN. \n",
        "### Output Layer: The layer that produces results for the data given in the input layer. \n",
        "### Neuron: A neuron receives one or more values of weighted input, and sums them by using an activation function, thus creating an output. \n",
        "### Weight: a value assigned to a neuron that modifies the input and reports an output. \n",
        "### Activation Function: a node defines the output of that node given an input or set of inputs\n",
        "### Node Map: a diagram of the strucure of a NN. Acts like a flow chart showing how we get our inputs to outputs. \n",
        "### Perceptron: Perceptrons are a single layer nueral network which acts as a binary classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "#### The flow is that you have input values (data), apply weights and a bias to those input values, summ those weighted input values, and then runn an activation function to get outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Z5_zeek6CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" IMPORTS \"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1EEyzyjU0gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "outputId": "172206a6-ab49-4b21-dbea-a557afbb4600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# Randomly initialize weights for the 3 inputs:\n",
        "weights = np.random.random((3, 1))\n",
        "\n",
        "# Input data\n",
        "df\n",
        "\n",
        "# Correct outputs\n",
        "correct_outputs = [[1], [1], [1], [0]]\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1 - sx)\n",
        "\n",
        "# update our weights 10,00 times \n",
        "for iteration in range(10000):\n",
        "\n",
        "  # calculate weighted sum of inputs / weights\n",
        "  weighted_sum = np.dot(df, weights)\n",
        "\n",
        "  # Activate the sigmoid function and output its value\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "\n",
        "  # Cac error by taking the difference bwteen output and correct values\n",
        "  error = correct_outputs - activated_output\n",
        "\n",
        "  adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "\n",
        "  # update the weights \n",
        "  weights += np.dot(df.T, adjustments)\n",
        "\n",
        "print(\"Weights after training:\")\n",
        "print(weights)\n",
        "\n",
        "print(\"Output after training:\")\n",
        "print(activated_output)"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights after training:\n",
            "[[-2.41050029]\n",
            " [-2.41001424]\n",
            " [ 7.49030754]]\n",
            "Output after training:\n",
            "[[0.9994418 ]\n",
            " [0.99381704]\n",
            " [0.99382003]\n",
            " [0.00799856]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gqd45MmU0gP",
        "colab_type": "code",
        "outputId": "3f199322-f210-4022-ffff-e25edcaf9e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLDp9vuU0gS",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSo2DxUU0gS",
        "colab_type": "code",
        "outputId": "d186cc59-8130-489c-9d32-ef6b9a3d550e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "\n",
        "feats = list(diabetes)[:-1]\n",
        "\n",
        "X = diabetes[feats]\n",
        "y = diabetes['Outcome']\n",
        "\n",
        "transformer = Normalizer()\n",
        "\n",
        "X_transformed = transformer.fit_transform(X)\n",
        "X_transformed = pd.DataFrame(X_transformed, columns=feats)\n",
        "X_transformed"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.033552</td>\n",
              "      <td>0.827625</td>\n",
              "      <td>0.402628</td>\n",
              "      <td>0.195722</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187893</td>\n",
              "      <td>0.003506</td>\n",
              "      <td>0.279603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.008424</td>\n",
              "      <td>0.716040</td>\n",
              "      <td>0.555984</td>\n",
              "      <td>0.244296</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.224079</td>\n",
              "      <td>0.002957</td>\n",
              "      <td>0.261144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.040398</td>\n",
              "      <td>0.924097</td>\n",
              "      <td>0.323181</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117658</td>\n",
              "      <td>0.003393</td>\n",
              "      <td>0.161591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.006612</td>\n",
              "      <td>0.588467</td>\n",
              "      <td>0.436392</td>\n",
              "      <td>0.152076</td>\n",
              "      <td>0.621527</td>\n",
              "      <td>0.185797</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.138852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.596386</td>\n",
              "      <td>0.174127</td>\n",
              "      <td>0.152361</td>\n",
              "      <td>0.731335</td>\n",
              "      <td>0.187622</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.143655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>0.042321</td>\n",
              "      <td>0.427443</td>\n",
              "      <td>0.321640</td>\n",
              "      <td>0.203141</td>\n",
              "      <td>0.761779</td>\n",
              "      <td>0.139236</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.266623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>0.013304</td>\n",
              "      <td>0.811526</td>\n",
              "      <td>0.465629</td>\n",
              "      <td>0.179600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.244788</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>0.026915</td>\n",
              "      <td>0.651352</td>\n",
              "      <td>0.387582</td>\n",
              "      <td>0.123811</td>\n",
              "      <td>0.602905</td>\n",
              "      <td>0.141037</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.161492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>0.006653</td>\n",
              "      <td>0.838285</td>\n",
              "      <td>0.399184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200257</td>\n",
              "      <td>0.002322</td>\n",
              "      <td>0.312694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>0.007915</td>\n",
              "      <td>0.736052</td>\n",
              "      <td>0.554018</td>\n",
              "      <td>0.245351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.240602</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>0.182034</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Pregnancies   Glucose  ...  DiabetesPedigreeFunction       Age\n",
              "0       0.033552  0.827625  ...                  0.003506  0.279603\n",
              "1       0.008424  0.716040  ...                  0.002957  0.261144\n",
              "2       0.040398  0.924097  ...                  0.003393  0.161591\n",
              "3       0.006612  0.588467  ...                  0.001104  0.138852\n",
              "4       0.000000  0.596386  ...                  0.009960  0.143655\n",
              "..           ...       ...  ...                       ...       ...\n",
              "763     0.042321  0.427443  ...                  0.000724  0.266623\n",
              "764     0.013304  0.811526  ...                  0.002262  0.179600\n",
              "765     0.026915  0.651352  ...                  0.001319  0.161492\n",
              "766     0.006653  0.838285  ...                  0.002322  0.312694\n",
              "767     0.007915  0.736052  ...                  0.002493  0.182034\n",
              "\n",
              "[768 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-W0tiX1F1hh2",
        "colab": {}
      },
      "source": [
        "##### Update this Class #####\n",
        "\n",
        "class Perceptron:\n",
        "    \n",
        "    def __init__(self, niter = 10):\n",
        "        self.niter = niter\n",
        "    \n",
        "    def __sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def __sigmoid_derivative(self, x):\n",
        "        sx = self.__sigmoid(x)\n",
        "        return sx * (1 - sx)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "      \"\"\"Fit training data\n",
        "      X : Training vectors, X.shape : [#samples, #features]\n",
        "      y : Target values, y.shape : [#samples]\n",
        "      \"\"\"\n",
        "\n",
        "        # Randomly Initialize Weights\n",
        "      self.weights = np.random.random((X.shape[1], 1))\n",
        "\n",
        "      for i in range(self.niter):\n",
        "            # Weighted sum of inputs / weights\n",
        "            self.weighted_sum = np.dot(X, self.weights)\n",
        "\n",
        "            # Activate!\n",
        "            self.activated_output = self.__sigmoid(self.weighted_sum)\n",
        "\n",
        "            # Cac error\n",
        "            self.error = y.values - self.activated_output\n",
        "            self.adjustments = self.error * self.__sigmoid_derivative(self.weighted_sum)\n",
        "\n",
        "            # Update the Weights\n",
        "            self.weights = self.weights + np.dot(np.array(X).T, self.adjustments)\n",
        "\n",
        "      return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "      \"\"\"Calculate net input\"\"\"\n",
        "      return np.dot(X, self.weights)\n",
        "\n",
        "    def predict(self, X):\n",
        "      \"\"\"Return class label after unit step\"\"\"\n",
        "      return np.where(self.net_input(X) >= 0.0, 1, -1)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJRt8k1JKPq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pn = Perceptron(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzqyoGhzKSV6",
        "colab_type": "code",
        "outputId": "87a19964-a7ed-43c5-e9d2-0afd0bd2a046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pn.fit(X_transformed, y)"
      ],
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7f8a27c8f5f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JqgBkvDKTqT",
        "colab_type": "code",
        "outputId": "ba7f8e3b-7f89-45d1-fa58-67d56da94a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "source": [
        "y_pred = pn.predict(X_transformed)\n",
        "y_pred"
      ],
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, -1,  1, -1,  1, -1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1,  1,\n",
              "        1, -1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1, -1,\n",
              "       -1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
              "       -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  1, -1,\n",
              "       -1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,\n",
              "       -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1,  1, -1, -1,\n",
              "       -1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "       -1,  1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1,\n",
              "        1,  1, -1, -1, -1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
              "        1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1, -1,\n",
              "       -1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
              "        1, -1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
              "        1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1, -1, -1, -1,  1, -1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1,\n",
              "       -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1, -1,\n",
              "       -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1,  1,  1, -1,  1, -1, -1,\n",
              "        1, -1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1, -1,  1, -1,  1,  1,\n",
              "        1, -1, -1,  1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
              "        1, -1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1,\n",
              "       -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,\n",
              "        1, -1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1, -1,\n",
              "        1,  1, -1, -1, -1, -1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
              "        1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1,\n",
              "       -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1,  1, -1, -1,  1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
              "        1, -1, -1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
              "        1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,\n",
              "        1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,\n",
              "       -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "       -1,  1,  1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1, -1,  1, -1, -1,\n",
              "        1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
              "        1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,\n",
              "        1,  1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
              "        1,  1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1,  1, -1,\n",
              "       -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1,  1,\n",
              "       -1, -1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1, -1,  1, -1,\n",
              "       -1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1,  1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1,  1, -1, -1,  1, -1,\n",
              "        1,  1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1, -1,\n",
              "       -1,  1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGI2R9sHuD8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eaf208d4-42a9-4a19-dd40-27cbb6c414d7"
      },
      "source": [
        "accuracy_score(y, y_pred)"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3489583333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}